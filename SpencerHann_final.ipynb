{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "Spencer Hann\n",
    "EE 584 - Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from cnn.data import preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data/mnist_train.csv... done. (20000, 785)\n",
      "loading data/mnist_test.csv... done. (100, 785)\n"
     ]
    }
   ],
   "source": [
    "training_examples, training_targets = \\\n",
    "    preprocess_data(\"data/mnist_train.csv\", max_rows=20000)\n",
    "validation_examples, validation_targets = \\\n",
    "    preprocess_data(\"data/mnist_test.csv\", max_rows=200)\n",
    "\n",
    "data = (training_examples, training_targets, validation_examples, validation_targets)\n",
    "train_set = (training_examples, training_targets)\n",
    "val_set = (validation_examples, validation_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof of Concept: Simple Neural Net Layer\n",
    "\n",
    "\n",
    "This section demonstrates, in a very simplified way, my approach to the problem of creating interconnectable, modular neural network layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn.cnn import CNN, Layer, DenseSoftmaxLayer, ConvolutionalLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(Layer):\n",
    "    def __init__(self, insize, outsize=10):\n",
    "        # layer dimensions\n",
    "        self.insize = insize\n",
    "        self.outsize = outsize\n",
    "\n",
    "        # random weight initializations\n",
    "        self.w = np.random.randn(insize, outsize) / insize\n",
    "        self.b = np.random.randn(outsize) / outsize\n",
    "\n",
    "    def forward(self, image):\n",
    "        image = image.flatten()  # dense layer requires a flat vector\n",
    "\n",
    "        # fully-connected/matmul phase + bias\n",
    "        result = np.dot(image, self.w) + self.b\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple nerual net fully connected layer that simply takes its input and multiplies it by its weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create example layers\n",
    "layer1 = DenseLayer(28*28, 64)\n",
    "layer2 = DenseLayer(64, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create multiple layers with matching `outsize` and `insize` dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(image, label):\n",
    "    middle = layer1.forward(image)\n",
    "    out = layer2.forward(middle)\n",
    "    \n",
    "    is_correct = np.argmax(out) == label\n",
    "    return None, is_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any number of `Layer` objects can be strung together so that each successive layer receives the outputs of the previous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(images, labels):\n",
    "    n_correct = 0.0\n",
    "    for image, label in zip(images, labels):\n",
    "        _, c = forward(image, label)\n",
    "        n_correct += c\n",
    "    return n_correct / len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 14.0%\n"
     ]
    }
   ],
   "source": [
    "accuracy = 100 * test(*val_set)  # should be about 1 / n_classes\n",
    "print(f\"Accuracy: {round(accuracy)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a randomly initialized network that we will test the feed forward functionality on.  With 10 evenly represented output classes in our testing data, we expect to see roughly 10% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(Layer):\n",
    "    def __init__(self, insize, outsize=10):\n",
    "        self.insize = insize\n",
    "        self.outsize = outsize\n",
    "\n",
    "        self.w = np.random.randn(insize, outsize) / insize\n",
    "        self.b = np.random.randn(outsize) / outsize\n",
    "\n",
    "    def forward(self, image):\n",
    "        self.last_image = image        # <<----\n",
    "        image = image.flatten()\n",
    "\n",
    "        # fully-connected/matmul phase\n",
    "        fc = np.dot(image, self.w) + self.b\n",
    "        self.last_fc = fc              # <<----\n",
    "\n",
    "        return fc\n",
    "\n",
    "    def backprop(self, loss_grad, lr):\n",
    "        # output gradients wrt input, biases, weights\n",
    "        ograd_input = self.w\n",
    "        ograd_biases = 1\n",
    "        ograd_weights = self.last_image.flatten()\n",
    "\n",
    "        # loss gradients wrt input, biases, weights\n",
    "        lgrad_input = ograd_input @ loss_grad\n",
    "        lgrad_biases = ograd_biases * loss_grad\n",
    "        lgrad_weights = ograd_weights[:,np.newaxis] @ loss_grad[np.newaxis]\n",
    "\n",
    "        # update layer\n",
    "        self.w += lr * lgrad_weights\n",
    "        self.b += lr * lgrad_biases\n",
    "        return lgrad_input.reshape(self.last_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CNN: lr=0.008, lr_decay=0.9, \n",
       "layers=(\t\n",
       "<ConvolutionalLayer: (1, 4, 3), momentum=0.0>,\t\n",
       "<DenseLayer>,\t\n",
       "<ReLULayer>,\t\n",
       "<DenseSoftmaxLayer: (64, 10)>\n",
       ")>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cnn.cnn import ReLULayer\n",
    "cnn = CNN((\n",
    "    ConvolutionalLayer(1,4,3),\n",
    "    DenseLayer(28*28*4, 64),\n",
    "    ReLULayer(),\n",
    "    DenseSoftmaxLayer(64,10)\n",
    "), lr=0.008, lr_decay=0.9)\n",
    "cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrap this example `DenseLayer` in a `CNN` object with some other layers. The CNN object allows thes packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 0/1...\n",
      "Epoch 0/5: 2.31 loss, 11.00% accurate, lr=0.008, \n",
      "Epoch 1/5: 2.31 loss, 19.75% accurate, lr=0.007200000000000001, \n",
      "Epoch 2/5: 2.31 loss, 21.00% accurate, lr=0.0064800000000000005, \n",
      "Epoch 3/5: 2.31 loss, 18.25% accurate, lr=0.005832, \n",
      "Epoch 4/5: 2.30 loss, 20.50% accurate, lr=0.0052488000000000005, \n",
      "Testing 0/1...\n",
      "Test: 2.30 loss, 18.00% accurate\n",
      "CPU times: user 6min 25s, sys: 4min 12s, total: 10min 38s\n",
      "Wall time: 2min 49s\n"
     ]
    }
   ],
   "source": [
    "%time cnn.train_test_cycle(5, 1, train_set, val_set, sample_size=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration of Full Networks\n",
    "\n",
    "The layers I built can be stacked in multiple ways to make arbitrary convolutional and fully-connected nerual architectures and trained. Each contains a `forward` function, and `backprop` function that lets them communicate loss gradients to each other during gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn.cnn import (\n",
    "    CNN, \n",
    "    ConvolutionalLayer, \n",
    "    MaxPoolingLayer, \n",
    "    DenseSoftmaxLayer,\n",
    "    ReLULayer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CNN: lr=0.01, lr_decay=0.9, \n",
       "layers=(\t\n",
       "<ConvolutionalLayer: (1, 3, 3), momentum=0.0>,\t\n",
       "<MaxPoolingLayer: 2>,\t\n",
       "<ConvolutionalLayer: (3, 1, 3), momentum=0.0>,\t\n",
       "<ReLULayer>,\t\n",
       "<DenseSoftmaxLayer: (196, 10)>\n",
       ")>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = CNN((\n",
    "    ConvolutionalLayer(1,3,3),\n",
    "    MaxPoolingLayer(2),\n",
    "    ConvolutionalLayer(3,1,3),\n",
    "    ReLULayer(),\n",
    "    DenseSoftmaxLayer(14*14, 10),\n",
    "))\n",
    "cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my `cnn.ConvolutionalLayer` class, the arguments are `(n_channels, n_filters, filter_size)`, currently only square filters are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 2.31 loss, 7.00% accurate\n"
     ]
    }
   ],
   "source": [
    "cnn.test(*val_set);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately random perfomance is to be expected.  This shows that feed forward is working properly, but that no learning has occured yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration of Back Propogation\n",
    "\n",
    "This section demonstrates support for multiple different neural architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_sample_size = 2000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def veiw_filters(cnn, layer = 0):  # only for filters w/ depth = 1 (first layer)\n",
    "    \"\"\"graphical view of what convolutional filters the network is learning.\"\"\"\n",
    "    plt.cla()\n",
    "    n = len(cnn.layers[layer].filters)\n",
    "    for i, fltr in enumerate(cnn.layers[layer].filters):\n",
    "        plt.subplot(1, n+1, i+1)\n",
    "        fltr -= fltr.min()\n",
    "        fltr /= fltr.max()\n",
    "        plt.imshow(fltr[0], cmap=\"gray\")\n",
    "        plt.xticks(()); plt.yticks(());\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CNN: lr=0.01, lr_decay=0.9, \n",
       "layers=(\t\n",
       "<ConvolutionalLayer: (1, 8, 3), momentum=0.0>,\t\n",
       "<DenseSoftmaxLayer: (6272, 10)>\n",
       ")>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = CNN((\n",
    "    ConvolutionalLayer(1,8,3),\n",
    "    DenseSoftmaxLayer(28*28*8, 10),\n",
    "))\n",
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAToAAAAxCAYAAABK8mUrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAD1klEQVR4nO3dzyt8exzH8fdcP8bU1NTwHcU0dxRZUH5kayNipVhZUNaKjD/AwsKGxILs/A8WiiLKz1IyUhYUkYZr4cfUFOncxd3d7xm9Jvf27X7u87E859XbuznTq5k6ZwQ8zzMAcNlvv3oBAPi3UXQAnEfRAXAeRQfAeRQdAOdRdACcV1xIOBAIyPeihMNhKVdfXy/lrq+v7enpKeB3LhqNevF4XJqTy+Wk3OXlpZQzM/M8z3evUCjkRSIRaUYg4DviJ9XV1VIu3+tVyDWsq6uTcqFQSB1p6XT6yfO8H38/HgwGPfU9U1ysvW0fHx/lvczMd6+KigovmUxKA87Pz6VcNBqVl7q/v//2XldXV1Lu7e1N3uvz89N3r/Lyci+RSEgzioqKpNzDw4O8193dne9eBRVdIVpbW6Xczs6OlGtra8t7Lh6P29ramjTn5OREyvX29kq5r0QiERscHJSypaWlUm56elrKffV6qRYXF6VcY2OjPLOqqurG73g4HLbu7m5pRiwWk3ILCwvyXmbmu1cymbTj42NpQENDg5QbGBiQl5qcnPz2Xv39/VJua2tL3uvl5cV3r0QiYdvb29IM9UPA/Py8vFcqlfLdi6+uAJxH0QFwHkUHwHkUHQDnUXQAnEfRAXAeRQfAeRQdAOcVdMNwJBKx9vZ2KXt6eirlJiYmpNzt7W3ec9ls1nZ3d6U56s2ahTyxkU8sFrOxsTFpzvr6upRTn6DIp7Ky0oaGhqRsV1eXlOvs7PzOSmb21zU8OjqSsk1NTVJOvTnczKylpcX3+MfHh2UyGWnG8PCwlFtaWlLXyiudTpv6BEJzc7OUK+TG7729Pd/jmUzGZmZmpBn7+/tSTr3h+St8ogPgPIoOgPMoOgDOo+gAOI+iA+A8ig6A8yg6AM6j6AA4j6ID4LyCnozIZrN2cHAgZVdWVqTc+Pi4lHt+fs577vX11TY2NqQ5q6urUm5kZETKzc7O5j13dnZmNTU10py5uTkpNzU1JeWWl5d9j+dyOUun09KMnp4eKVfIneubm5u+x9/f3+3mxvdXsH+i/g+Ei4sLea+v/lZfX5+UPTw8lHLqtTbL/+RQSUmJ/JPyHR0dUi6VSsl75VNWVma1tbVSNhgMSrnR0dHvrGRmfKID8D9A0QFwHkUHwHkUHQDnUXQAnEfRAXAeRQfAeRQdAOdRdACcR9EBcF7A8zw9HAj8YWbaczr/vN89z/vhd4K9fPnu9Yt3MmOvQrFXYfz3KqToAOC/iK+uAJxH0QFwHkUHwHkUHQDnUXQAnEfRAXAeRQfAeRQdAOdRdACc9yc/kR0W4cpevgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "veiw_filters(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 0/2...\n",
      "Epoch 0/3: 2.12 loss, 49.30% accurate, lr=0.01, \n",
      "Epoch 1/3: 1.55 loss, 62.15% accurate, lr=0.009000000000000001, \n",
      "Epoch 2/3: 1.58 loss, 62.60% accurate, lr=0.008100000000000001, \n",
      "Testing 0/2...\n",
      "Test: 1.50 loss, 64.00% accurate\n",
      "Training 1/2...\n",
      "Epoch 0/3: 1.64 loss, 61.75% accurate, lr=0.007290000000000001, \n",
      "Epoch 1/3: 1.60 loss, 62.05% accurate, lr=0.006561000000000002, \n",
      "Epoch 2/3: 1.61 loss, 63.05% accurate, lr=0.005904900000000002, \n",
      "Testing 1/2...\n",
      "Test: 1.46 loss, 67.00% accurate\n",
      "CPU times: user 41min 39s, sys: 26min 35s, total: 1h 8min 15s\n",
      "Wall time: 19min 43s\n"
     ]
    }
   ],
   "source": [
    "%time cnn.train_test_cycle(3, 2, train_set, val_set, sample_size=def_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAToAAAAxCAYAAABK8mUrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAD3UlEQVR4nO3dzSu8exzG8c8cD2Nqagq/UUxzKLKgPGRrI2KlyMKCslZkZG1hYeMhFmTnf7BQRFEeS8lIWVDUpOEoeagp0n0WZ3d+9+ianNOv8z3v1/K+rz7zaWa6mqn7ngl4nmcA4LLffvUCAPBvo+gAOI+iA+A8ig6A8yg6AM6j6AA4Lz+XcCAQkK9FCYfDUq62tlbK3dzc2OPjY8DvXHFxsReLxaQ5mUxGyl1dXUk5MzPP83z3CoVCXiQSkWYEAr4jflJRUSHlsj1fubyGNTU1Ui4UCqkjLZlMPnqe9+Pvx4PBoKe+Z/Lztbftw8ODvJeZ+e5VWlrqVVZWSgMuLi6kXHFxsbzU3d3dt/e6vr6Wcq+vr/Jen5+fvnuVlJR48XhcmpGXlyfl7u/v5b1SqZTvXjkVXS6am5ul3O7urpRraWnJei4Wi9n6+ro05/T0VMp1d3dLua9EIhEbGBiQsoWFhVJuenpayn31fKmWlpakXH19vTyzvLz81u94OBy2zs5OaUY0GpVyi4uL8l5m5rtXZWWlnZycSAPq6uqkXH9/v7zU5OTkt/fq6+uTcltbW/Jez8/PvnvF43Hb2dmRZqgfAhYWFuS9EomE7158dQXgPIoOgPMoOgDOo+gAOI+iA+A8ig6A8yg6AM6j6AA4L6cLhiORiLW2tkrZs7MzKTcxMSHlUqlU1nNvb2+2t7cnzVEv1szljo1sotGojY6OSnM2NjaknHoHRTZlZWU2ODgoZTs6OqRce3v7d1Yys79ew+PjYynb0NAg5dSLw83MmpqafI9/fHxYOp2WZgwNDUm55eVlda2sksmkqXcgNDY2SrlcLvze39/3PZ5Op21mZkaacXBwIOV6e3vlvbLhEx0A51F0AJxH0QFwHkUHwHkUHQDnUXQAnEfRAXAeRQfAeRQdAOfldGfE29ubHR4eStnV1VUpNzY2JuWenp6ynnt5ebHNzU1pztrampQbHh6WcrOzs1nPnZ+fW1VVlTRnbm5Oyk1NTUm5lZUV3+OZTMaSyaQ0o6urS8rlcuX69va27/H393e7vfX9FeyfqP+BcHl5Ke/11WP19PRI2aOjIyk3Pz8vP/74+Ljv8YKCAvkn5dva2qRcIpGQ98qmqKjIqqurpWwwGJRyIyMj31nJzPhEB+B/gKID4DyKDoDzKDoAzqPoADiPogPgPIoOgPMoOgDOo+gAOI+iA+C8gOd5ejgQ+MPMtPt0/nm/e573w+8Ee/ny3esX72TGXrlir9z475VL0QHAfxFfXQE4j6ID4DyKDoDzKDoAzqPoADiPogPgPIoOgPMoOgDOo+gAOO9PQDwdGaeuwoAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "veiw_filters(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CNN: lr=0.01, lr_decay=0.9, \n",
       "layers=(\t\n",
       "<ConvolutionalLayer: (1, 3, 3), momentum=0.0>,\t\n",
       "<MaxPoolingLayer: 2>,\t\n",
       "<ConvolutionalLayer: (3, 3, 3), momentum=0.0>,\t\n",
       "<DenseSoftmaxLayer: (588, 10)>\n",
       ")>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = CNN((\n",
    "    ConvolutionalLayer(1,3,3),\n",
    "    MaxPoolingLayer(2),\n",
    "    ConvolutionalLayer(3,3,3),\n",
    "    DenseSoftmaxLayer(14*14*3, 10),\n",
    "))\n",
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 0/2...\n",
      "Epoch 0/3: 2.31 loss, 10.75% accurate, lr=0.01, \n",
      "Epoch 1/3: 2.26 loss, 47.55% accurate, lr=0.009000000000000001, \n",
      "Epoch 2/3: 2.18 loss, 68.45% accurate, lr=0.008100000000000001, \n",
      "Testing 0/2...\n",
      "Test: 2.12 loss, 71.00% accurate\n",
      "Training 1/2...\n",
      "Epoch 0/3: 2.04 loss, 72.90% accurate, lr=0.007290000000000001, \n",
      "Epoch 1/3: 1.89 loss, 72.60% accurate, lr=0.006561000000000002, \n",
      "Epoch 2/3: 1.76 loss, 74.55% accurate, lr=0.005904900000000002, \n",
      "Testing 1/2...\n",
      "Test: 1.74 loss, 76.00% accurate\n",
      "CPU times: user 5min 18s, sys: 120 ms, total: 5min 18s\n",
      "Wall time: 5min 18s\n"
     ]
    }
   ],
   "source": [
    "%time cnn.train_test_cycle(3, 2, train_set, val_set, sample_size=def_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQkAAABaCAYAAACrBaOJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAACwUlEQVR4nO3dsUprWRiG4RWxSMQECZzWacUbsLS2shEsBbG19SpEO8HOW/AS7CzEWu0kWMgcwcJKhX2qfUCYfJhNFmeGeZ46699/VvGSNEmvaZoCMM3Cn14A+HcTCSASCSASCSASCSASCSBanOXFS0tLzWg0qrVLeX19rTa7tbBQr4vv7+/l8/Oz1+XsYDBohsPhvFf6rebs1srKStX5t7e3P5um+THrueXl5WY8HtdYqZRSysvLS7XZrdXV1arz7+7upt7tTJEYjUZlb29vPlv9g8vLy2qzW/1+v9rsh4eHzmeHw2HZ2dmZ4zZfbW5uVpvd2t7erjp/MBg8djk3Ho/L0dHRvNf57eLiotrs1tnZWdX5GxsbU+/W1w0gEgkgEgkgEgkgEgkgEgkgEgkgEgkgEgkgEgkgEgkgEgkgEgkgEgkgEgkgEgkgEgkgEgkgEgkgEgkgEgkgmunXst/e3srV1VWtXcr9/X212a2Dg4Nqs5+enjqf7ff7ZX19fY7bfLW7u1ttduvxsdOPWVfXNE35+PioNv/m5qba7NbJyUn1Z0zjkwQQiQQQiQQQiQQQiQQQiQQQiQQQiQQQiQQQiQQQiQQQiQQQiQQQiQQQiQQQiQQQiQQQiQQQiQQQiQQQiQQQiQQQiQQQiQQQzfTnPGtra+X6+rrWLmV/f7/a7Nbi4kxveSa9Xq/z2clkUg4PD+e4zVdbW1vVZrfOz8+rP6OL5+fncnx8XG3+ZDKpNrt1enpa/RnT+CQBRCIBRCIBRCIBRCIBRCIBRCIBRCIBRCIBRCIBRCIBRCIBRCIBRCIBRCIBRCIBRCIBRCIBRCIBRCIBRCIBRCIBRCIBRL2mab7/4l7v71LKY711/vP+aprmR5eD7vZbOt2vu/2WqXc7UySA/x9fN4BIJIBIJIBIJIBIJIBIJIBIJIBIJIBIJIDoF0toc0kYuWpEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "veiw_filters(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CNN: lr=0.01, lr_decay=0.9, \n",
       "layers=(\t\n",
       "<ConvolutionalLayer: (1, 8, 3), momentum=0.0>,\t\n",
       "<ConvolutionalLayer: (8, 4, 3), momentum=0.0>,\t\n",
       "<DenseSoftmaxLayer: (3136, 10)>\n",
       ")>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = CNN((\n",
    "    ConvolutionalLayer(1,8,3),\n",
    "    ConvolutionalLayer(8,4,3),\n",
    "    DenseSoftmaxLayer(28*28*4, 10),\n",
    "))\n",
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 0/2...\n",
      "Epoch 0/3: 1.92 loss, 45.45% accurate, lr=0.01, \n",
      "Epoch 1/3: 1.37 loss, 67.55% accurate, lr=0.009000000000000001, \n",
      "Epoch 2/3: 1.35 loss, 71.55% accurate, lr=0.008100000000000001, \n",
      "Testing 0/2...\n",
      "Test: 1.34 loss, 73.00% accurate\n",
      "Training 1/2...\n",
      "Epoch 0/3: 1.40 loss, 68.40% accurate, lr=0.007290000000000001, \n",
      "Epoch 1/3: 1.43 loss, 68.55% accurate, lr=0.006561000000000002, \n",
      "Epoch 2/3: 1.44 loss, 69.05% accurate, lr=0.005904900000000002, \n",
      "Testing 1/2...\n",
      "Test: 1.41 loss, 68.00% accurate\n",
      "CPU times: user 50min 45s, sys: 29min 57s, total: 1h 20min 43s\n",
      "Wall time: 25min 43s\n"
     ]
    }
   ],
   "source": [
    "%time cnn.train_test_cycle(3, 2, train_set, val_set, sample_size=def_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAToAAAAxCAYAAABK8mUrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAADrElEQVR4nO3dwStsfxzG8c/oNs2lSCGJMWtRslDKDhtWNmz9CYqyIPbyB7CQyELWykIhC2ZhFmbHSDSy+TGSwbA5v4Wde0bPafy6/T69X8vx9L1PY+7TqDlnYkEQGAB4VvW3CwDAf42hA+AeQwfAPYYOgHsMHQD3GDoA7v2KEm5oaAhSqZSUfXx8lHI1NTVS7vb21gqFQqzSXtfX11Lu4eFBypmZBUEQ2isejweJREI6o7q6WsrF43EpVygUrFgs/tGroaEhSCaT0hmvr69S7uLiQsqZmQVBcB8EQePXx2OxmPw5p46ODin3+/dvuVcmk6m4V319vZT7+PiQe728vIT2ivKav7q6knJRnq+7u7uyvdTX1+XlpZRrbm6We+VyudBekYYulUrZ6emplN3e3pZyfX19Um5kZORHek1MTEi59fV1KfedRCJhvb29Uranp0fKtba2SrmlpaXQx5PJpB0dHUlnZLNZKTcwMCDlzMxKpdKNHC5ja2tLynV1dclnxmKxinsNDg5KuXw+L5+ZTqdDe0V5zY+NjUm5KM/X/Px8aK8or6/R0VEpNzMzI/caGhoK7cWfrgDcY+gAuMfQAXCPoQPgHkMHwD2GDoB7DB0A9xg6AO5F+sBwPp+3yclJKVssFqXc+Ph4lAqhzs7OrLHxjw9Dh9rc3JTPVJyfn5f9WV1dnQ0PD0vnTE1NSTn1RqkbGxuhj+fzeZuenpbOWFlZ+dFOZmaxWOhFJNbe3m5zc3PSGel0Wsqtra3Jvcrp7u62g4MDKat+APbk5ET+98s9X5lMpuzPvlJ/j01NTXKvcrLZrKlXbHR2dkq5t7e3Chp94h0dAPcYOgDuMXQA3GPoALjH0AFwj6ED4B5DB8A9hg6AewwdAPciXRlxf39vq6urUnZ3d1fKZTIZKffdFQgtLS02OzsrnXNzo90xW/1uhu8+nV4sFu34+Fg6R71F+v7+vpR7fn4Ofbytra3sbda/2tvbk3LqLcS/UyqVLJfLSVn1lt+1tbWVVDIzs6enJ9vZ2ZGyy8vLUm5xcbGSSmb2eSv1hYUFKfv+/i7l+vv7K6lkZp//H6qqtPdPh4eH8pmV4h0dAPcYOgDuMXQA3GPoALjH0AFwj6ED4B5DB8A9hg6AewwdAPcYOgDuxSJ+sck/ZqZdQ/Xz2oMgCP0GHHqFCu31lzuZ0SsqekUT3ivK0AHA/xF/ugJwj6ED4B5DB8A9hg6AewwdAPcYOgDuMXQA3GPoALjH0AFw71+q7CIylASYvgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "veiw_filters(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CNN: lr=0.01, lr_decay=0.8, \n",
       "layers=(\t\n",
       "<ConvolutionalLayer: (1, 16, 3), momentum=0.0>,\t\n",
       "<MaxPoolingLayer: 2>,\t\n",
       "<ReLULayer>,\t\n",
       "<ConvolutionalLayer: (16, 32, 3), momentum=0.0>,\t\n",
       "<MaxPoolingLayer: 2>,\t\n",
       "<ReLULayer>,\t\n",
       "<DenseSoftmaxLayer: (1568, 10)>\n",
       ")>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = CNN(\n",
    "    (\n",
    "        ConvolutionalLayer(1,16,3),\n",
    "        MaxPoolingLayer(2),\n",
    "        ReLULayer(),\n",
    "        ConvolutionalLayer(16,32,3),\n",
    "        MaxPoolingLayer(2),\n",
    "        ReLULayer(),\n",
    "        DenseSoftmaxLayer(7*7*32, 10),\n",
    "    ), \n",
    "    lr = 0.01,\n",
    "    lr_decay = 0.8,\n",
    ")\n",
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 0/3...\n",
      "Epoch 0/3: 2.30 loss, 19.67% accurate, lr=0.01, \n",
      "Epoch 1/3: 2.30 loss, 48.80% accurate, lr=0.008, \n",
      "Epoch 2/3: 2.30 loss, 63.73% accurate, lr=0.0064, \n",
      "Testing 0/3...\n",
      "Test: 2.30 loss, 67.00% accurate\n",
      "Training 1/3...\n",
      "Epoch 0/3: 2.30 loss, 67.67% accurate, lr=0.00512, \n",
      "Epoch 1/3: 2.30 loss, 72.07% accurate, lr=0.004096000000000001, \n",
      "Epoch 2/3: 2.30 loss, 74.37% accurate, lr=0.0032768000000000007, \n",
      "Testing 1/3...\n",
      "Test: 2.30 loss, 78.00% accurate\n",
      "Training 2/3...\n",
      "Epoch 0/3: 2.30 loss, 74.70% accurate, lr=0.002621440000000001, \n",
      "Epoch 1/3: 2.30 loss, 76.67% accurate, lr=0.002097152000000001, \n",
      "Epoch 2/3: 2.30 loss, 77.10% accurate, lr=0.001677721600000001, \n",
      "Testing 2/3...\n",
      "Test: 2.30 loss, 77.00% accurate\n",
      "CPU times: user 2h 32min 22s, sys: 1h 3min 34s, total: 3h 35min 57s\n",
      "Wall time: 1h 39min 21s\n"
     ]
    }
   ],
   "source": [
    "%time cnn.train_test_cycle(3, 3, train_set, val_set, sample_size=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "veiw_filters(cnn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
